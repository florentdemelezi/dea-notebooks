{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3 FC percentile validation\n",
    "\n",
    "* [**Sign up to the DEA Sandbox**](https://docs.dea.ga.gov.au/setup/sandbox.html) to run this notebook interactively from a browser\n",
    "* **Compatibility:** Notebook currently compatible with the`DEA Sandbox` environments\n",
    "* **Products used:** \n",
    "[fc_percentile_albers_annual](https://explorer.sandbox.dea.ga.gov.au/products/fc_percentile_albers_annual), \n",
    "C3 fc percentile test product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The notebook is to validate the new C3 fc percentile product against the C2 product `fc_percentile_albers_annual`. It produced the output for the validation report.\n",
    "\n",
    "1. Generate distritubtions and plot PDFs as the validation results\n",
    "2. Produce the summary of validation results\n",
    "3. Plot examples of the findings\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Install the package needed by\n",
    "\n",
    "`!pip install awswrangler`\n",
    "\n",
    "in the top cell or the terminal then restart notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "import rasterio\n",
    "import boto3\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import re\n",
    "from datacube.utils.dask import start_local_dask\n",
    "from datacube import Datacube\n",
    "from datacube.utils.geometry import CRS, Geometry, GeoBox\n",
    "from osgeo import ogr, gdal, osr\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.stats as sps\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import fiona\n",
    "\n",
    "from odc.algo.io import load_with_native_transform\n",
    "from odc.algo import keep_good_only\n",
    "from odc.algo._masking import _xr_fuse, _or_fuser, _fuse_mean_np, _fuse_or_np, _fuse_and_np, enum_to_bool\n",
    "from odc.stats.utils import fuse_ds, fuse_products\n",
    "from functools import partial\n",
    "from itertools import groupby\n",
    "\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local cluster\n",
    "client = start_local_dask(n_workers=1, threads_per_worker=60, memory_limit='478GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `dev` is the credential profile name\n",
    "# change it accordingly\n",
    "session = boto3.Session(profile_name='dev')\n",
    "fc_bucket = \"s3://dea-public-data-dev/test/fc-percentile/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_list = pd.read_csv(\"../../../grid_test_fc_percentile_mangroves.csv\")\n",
    "test_grids = grid_list[\"New grid\"]\n",
    "dc = Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_from_region_code(region_code, grids_file):\n",
    "    with fiona.open(grids_file) as allshapes:\n",
    "        for shape in allshapes:\n",
    "            if shape['properties'].get('region_code', '') == region_code.lower():\n",
    "                return Geometry(shape['geometry'], crs=CRS('EPSG:4326'))\n",
    "    \n",
    "def generate_seamask(shape_file, data_shape, orig_coords, resolution):\n",
    "    \"\"\"\n",
    "        creak mask without oceans\n",
    "        input:\n",
    "            shape_file: the shape file of Australia coastline\n",
    "            data_shape: the shape of loaded data to be masked upon\n",
    "            orig_coords: the origin of the image for gdal to decide the transform\n",
    "            resolution: pixel size with signs, e.g., (30, -30) for C3 and (25, -25) for C2\n",
    "        output:\n",
    "            a numpy array of mask, where valid pixels = 1\n",
    "    \"\"\"\n",
    "    source_ds = ogr.Open(shape_file)\n",
    "    source_layer = source_ds.GetLayer()\n",
    "    source_layer.SetAttributeFilter(\"FEAT_CODE!='sea'\")\n",
    "\n",
    "    yt, xt = data_shape\n",
    "    xres = resolution[0]\n",
    "    yres = resolution[1]\n",
    "    no_data = 0\n",
    "\n",
    "    xcoord, ycoord = orig_coords\n",
    "    geotransform = (xcoord - (xres*0.5), xres, 0, ycoord - (yres*0.5), 0, yres)\n",
    "\n",
    "    target_ds = gdal.GetDriverByName('MEM').Create('', xt, yt, gdal.GDT_Byte)\n",
    "    target_ds.SetGeoTransform(geotransform)\n",
    "    albers = osr.SpatialReference()\n",
    "    albers.ImportFromEPSG(3577)\n",
    "    target_ds.SetProjection(albers.ExportToWkt())\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(no_data)\n",
    "\n",
    "    gdal.RasterizeLayer(target_ds, [1], source_layer, burn_values=[1])\n",
    "    return band.ReadAsArray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    def _native_tr(xx):\n",
    "        \"\"\"\n",
    "        Loads data in its native projection. It performs the following:\n",
    "\n",
    "        1. Load all fc and WOfS bands\n",
    "        2. Set the high terrain slope flag to 0\n",
    "        3. Set all pixels that are not clear and dry to NODATA\n",
    "        4. Calculate the clear wet pixels\n",
    "        5. Drop the WOfS band\n",
    "        \"\"\"\n",
    "        water = xx.water & 0b1110_1111\n",
    "        dry = water == 0\n",
    "        xx = xx.drop_vars([\"water\"])\n",
    "        xx = keep_good_only(xx, dry, nodata=255)\n",
    "        return xx\n",
    "\n",
    "    def _fuser(xx):\n",
    "        xx = _xr_fuse(xx, partial(_fuse_mean_np, nodata=255), '')\n",
    "        return xx\n",
    "    \n",
    "    def filter(groups, size=2):\n",
    "        for _, ds_group in groups:\n",
    "            ds_group = tuple(ds_group)\n",
    "            if len(ds_group) == size:\n",
    "                yield ds_group\n",
    "    \n",
    "    def ds_align(datasets):\n",
    "        datasets.sort(key=lambda ds: (ds.center_time, ds.metadata.region_code))\n",
    "        paired_dss = groupby(datasets, key=lambda ds: (ds.center_time, ds.metadata.region_code))\n",
    "        paired_dss = filter(paired_dss)\n",
    "        map_fuse_func = lambda x: fuse_ds(*x)\n",
    "        dss = map(map_fuse_func, paired_dss)\n",
    "        return dss\n",
    "    \n",
    "    def _fuser_nbart(xx):\n",
    "        xx = _xr_fuse(xx, partial(_fuse_mean_np, nodata=-999), '')\n",
    "        return xx\n",
    "    \n",
    "    def _native_tr_nbart(xx):\n",
    "        \"\"\"\n",
    "        Loads data in its native projection.\n",
    "        \"\"\"\n",
    "        bad = enum_to_bool(xx[\"fmask\"], (\"nodata\", \"cloud\", \"shadow\", \"water\")) # a pixel is bad if any of the cloud, shadow, or no-data value\n",
    "        bad |= xx[\"nbart_contiguity\"] == 0 # or the nbart contiguity bit is 0\n",
    "        xx = xx.drop_vars([\"fmask\", \"nbart_contiguity\"])\n",
    "        \n",
    "        for band in xx.data_vars.keys():\n",
    "            bad = bad | (xx[band] == -999)\n",
    "        xx = keep_good_only(xx, ~bad, nodata=-999)\n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_yearly_diff(grid, ls8_data, ls7_data, years, band, collection):\n",
    "    data_to_plot = []\n",
    "    fig, axs = plt.subplots(1, 1,  sharey=True, sharex=True, figsize=(8, 8))\n",
    "    pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "    for p in pcs:\n",
    "        data_to_plot = []\n",
    "        for y in range(years[0], years[1]):\n",
    "            data_to_plot += [ls8_data[band].loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')].median() - \n",
    "                             ls7_data[band].loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')].median()]\n",
    "        axs.plot(np.arange(years[0], years[1]), data_to_plot, label=\"Median difference on %s percentile\" %  int(p*100))\n",
    "    axs.axhline(y=0, color='black', linestyle='dashdot')\n",
    "    plt.tight_layout()\n",
    "    fig.legend(loc='upper left', ncol=3)\n",
    "    plt.savefig('fc_diff_plot/' + '_'.join([grid, band, collection]) + '_yearly_median_diff.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alltime_box(grid, ls8_data, ls7_data, band, collection):\n",
    "    fig, axs= plt.subplots(figsize=(10, 6))\n",
    "    pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "    positions = np.arange(1, (len(pcs) + 1), 1.0)\n",
    "    labels_ls7 = ['LS7-'+str(int(p*100)) for p in pcs]\n",
    "    data_to_plot = []\n",
    "    for p in pcs:\n",
    "        data_to_plot += [ls7_data[band].loc[p]]\n",
    "    box_plot_ls7 = axs.boxplot(data_to_plot, vert=1, widths=0.3, patch_artist=True, showfliers=False,\n",
    "                  positions=positions,\n",
    "                  labels=labels_ls7)\n",
    "    \n",
    "    positions += 0.4\n",
    "    labels_ls8 = ['LS8-'+str(int(p*100)) for p in pcs]\n",
    "    data_to_plot = []\n",
    "    for p in pcs:\n",
    "        data_to_plot += [ls8_data[band].loc[p]]\n",
    "    box_plot_ls8 = axs.boxplot(data_to_plot, vert=1, widths=0.3, showfliers=False,\n",
    "                  positions=positions,\n",
    "                  labels=labels_ls8)\n",
    "    axs.set_xticklabels(labels_ls7+labels_ls8,\n",
    "                    rotation=45, fontsize=8)\n",
    "    for item in ['boxes', 'whiskers', 'caps']:\n",
    "        plt.setp(box_plot_ls7[item], color='darkblue')\n",
    "    plt.savefig('fc_diff_plot/' + '_'.join([grid, band, collection]) + '_alltime_diff.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = {\"y\": -1, \"x\": -1}\n",
    "first = False\n",
    "pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "for grid in test_grids:\n",
    "    print(\"grid\", grid)\n",
    "    if grid.lower() != 'x39y13' and grid.lower() != 'x40y13':\n",
    "        continue\n",
    "    # if grid.lower() != 'x35y22' and not first:\n",
    "    #    continue\n",
    "    # if grid.lower() == 'x35y22':\n",
    "    #    first = True\n",
    "    query_poly = poly_from_region_code(grid, \"../../../au-grid.geojson\")\n",
    "    c3_query = {'geopolygon': query_poly}\n",
    "    c3_query['time'] = ('2014-01-01', '2021-01-01')\n",
    "    geobox = GeoBox.from_geopolygon(query_poly, (-30, 30), crs='epsg:3577')\n",
    "    \n",
    "    c3_ls7_datasets = dc.find_datasets(product=['ga_ls_wo_3', 'ga_ls_fc_3'], **c3_query,\n",
    "                 platform=\"landsat-7\", group_by=\"solar_day\")\n",
    "    c3_ls7_datasets = ds_align(c3_ls7_datasets)\n",
    "    c3_ls7 = load_with_native_transform(\n",
    "        c3_ls7_datasets,\n",
    "        bands=[\"water\", \"pv\", \"bs\", \"npv\"],\n",
    "        geobox=geobox,\n",
    "        native_transform=_native_tr,\n",
    "        fuser=_fuser,\n",
    "        groupby=\"solar_day\",\n",
    "        resampling=\"bilinear\",\n",
    "        chunks=chunks,\n",
    "    )\n",
    "    c3_land_raster = generate_seamask(\"../../../aus_map/cstauscd_r_3577.shp\",\n",
    "                                      c3_ls7.pv.shape[1:], (c3_ls7.x.data.min(), c3_ls7.y.data.max()), (30, -30))\n",
    "\n",
    "    c3_ls8_datasets = dc.find_datasets(product=['ga_ls_fc_3', 'ga_ls_wo_3'], **c3_query,\n",
    "                 platform=\"landsat-8\", group_by=\"solar_day\")\n",
    "    c3_ls8_datasets = ds_align(c3_ls8_datasets)\n",
    "    c3_ls8 = load_with_native_transform(\n",
    "        c3_ls8_datasets,\n",
    "        bands=[\"water\", \"pv\", \"bs\", \"npv\"],\n",
    "        geobox=geobox,\n",
    "        native_transform=_native_tr,\n",
    "        fuser=_fuser,\n",
    "        groupby=\"solar_day\",\n",
    "        resampling=\"bilinear\",\n",
    "        chunks=chunks,\n",
    "    )\n",
    "    c3_ls7 = c3_ls7.where((c3_ls7 < 255) & c3_land_raster)\n",
    "    c3_ls8 = c3_ls8.where((c3_ls8 < 255) & c3_land_raster)\n",
    "    ls7_pc_10 = c3_ls7.quantile(pcs, dim=['x', 'y'], skipna=True).compute().dropna('spec', how='all')\n",
    "    ls8_pc_10 = c3_ls8.quantile(pcs, dim=['x', 'y'], skipna=True).compute().dropna('spec', how='all')\n",
    "    ls7_pc_10 = ls7_pc_10.reset_index(['time', 'idx', 'uuid', 'grid'], drop=True).rename({'spec': 'time'}).to_dataframe()\n",
    "    ls8_pc_10 = ls8_pc_10.reset_index(['time', 'idx', 'uuid', 'grid'], drop=True).rename({'spec': 'time'}).to_dataframe()\n",
    "    ls7_pc_10.to_csv(grid.lower() + '_fc_ls7_c3.csv')\n",
    "    ls8_pc_10.to_csv(grid.lower() + '_fc_ls8_c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for grid in test_grids:\n",
    "    grid = grid.lower()\n",
    "    ls8_pc_10 = pd.read_csv('fc_diff_data/'+grid+'_fc_ls8_c3.csv')\n",
    "    ls8_pc_10['time'] = ls8_pc_10['time'].astype(np.datetime64)\n",
    "    ls8_pc_10 = ls8_pc_10.set_index(['quantile', 'time'])\n",
    "    ls7_pc_10 = pd.read_csv('fc_diff_data/'+grid+'_fc_ls7_c3.csv')\n",
    "    ls7_pc_10['time'] = ls7_pc_10['time'].astype(np.datetime64)\n",
    "    ls7_pc_10 = ls7_pc_10.set_index(['quantile', 'time'])\n",
    "    plot_yearly_diff(grid, ls8_pc_10, ls7_pc_10, (2014, 2021), 'pv', 'c3')\n",
    "    plot_alltime_box(grid, ls8_pc_10, ls7_pc_10,'pv', 'c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_median_ls8 = []\n",
    "max_median_ls7 = []\n",
    "pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "for grid in test_grids:\n",
    "    grid = grid.lower()\n",
    "    ls8_pc_10 = pd.read_csv('fc_diff_data/'+grid+'_fc_ls8_c3.csv')\n",
    "    ls8_pc_10['time'] = ls8_pc_10['time'].astype(np.datetime64)\n",
    "    ls8_pc_10 = ls8_pc_10.set_index(['quantile', 'time'])\n",
    "    ls7_pc_10 = pd.read_csv('fc_diff_data/'+grid+'_fc_ls7_c3.csv')\n",
    "    ls7_pc_10['time'] = ls7_pc_10['time'].astype(np.datetime64)\n",
    "    ls7_pc_10 = ls7_pc_10.set_index(['quantile', 'time'])\n",
    "    for p in pcs:\n",
    "        max_median_ls8 += [ls8_pc_10.loc[p].pv.median()]\n",
    "        max_median_ls7 += [ls7_pc_10.loc[p].pv.median()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_lin = linregress(max_median_ls8, max_median_ls7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1,  sharey=True, sharex=True, figsize=(9, 6))\n",
    "axs.plot(max_median_ls8, max_median_ls7, 'o',  color='SteelBlue',  mfc='none', markersize=5)\n",
    "axs.set_xlabel(\"LS8\")\n",
    "axs.set_ylabel(\"LS7\")\n",
    "axs.axline([0, 0], [1, 1], color='darkgreen', ls='--', label=\"1:1\")\n",
    "axs.plot(max_median_ls8, fc_lin.slope*np.array(max_median_ls8)+fc_lin.intercept, color='darkorange', label='regression')\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper left', ncol=1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fc_diff_plot/' + 'fc_regression.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_grids[np.argsort(max_median)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3,  sharey=True, sharex=True, figsize=(15, 15))\n",
    "i = 0\n",
    "j = 0\n",
    "year = 2014\n",
    "pcs = [0.2, 0.5, 0.8]\n",
    "for y in range(2014, 2021):\n",
    "    data_to_plot = []\n",
    "    for p in pcs:\n",
    "        data_to_plot += [ls7_pc.pv.loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')],\n",
    "                         ls8_pc.pv.loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')]]\n",
    "    box_plot = axs[i, j].boxplot(data_to_plot,\n",
    "                  positions=[1, 1.6, 2.5, 3.1, 4, 4.6],\n",
    "                  labels=['LS7-20','LS8-20','LS7-50','LS8-50','LS7-80','LS8-80'])\n",
    "    axs[i, j].set_title(str(y))\n",
    "    if j >= 2:\n",
    "        i += 1\n",
    "        j = 0\n",
    "    else:\n",
    "        j += 1\n",
    "plt.savefig(test_grids[0] + '_pv_c3_yearly_diff.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grid in test_grids:\n",
    "    query_poly = poly_from_region_code(grid, \"../../../au-grid.geojson\")\n",
    "    c3_query = {'geopolygon': query_poly}\n",
    "    c3_query['time'] = ('2014-01-01', '2020-01-01')\n",
    "    c2_ls7 = dc.load(product=['ls7_fc_albers'], **c3_query, group_by=\"solar_day\", measurements=['BS', 'PV', 'NPV'], \n",
    "                     dask_chunks={'time':1})\n",
    "    c2_wofs = dc.load(product=['wofs_albers'], **c3_query, group_by=\"solar_day\", platform='LANDSAT_7', measurements=['water'],\n",
    "                      dask_chunks={'time':1})\n",
    "    c2_land_raster = generate_seamask(\"../../../aus_map/cstauscd_r_3577.shp\", c2_ls7.PV.shape[1:],\n",
    "                                      (c2_ls7.x.data.min(), c2_ls7.y.data.max()), (25, -25))\n",
    "    dates = np.intersect1d(c2_ls7.time.data, c2_wofs.time.data)\n",
    "    water = c2_wofs.water & 0b1110_1011\n",
    "    dry = water == 0\n",
    "    c2_ls7 = c2_ls7.loc[dict(time=dates)]\n",
    "    dry = dry.loc[dict(time=dates)]\n",
    "    c2_ls7 = c2_ls7.where((c2_ls7 > -1) & dry & c2_land_raster)\n",
    "\n",
    "    c2_ls8 = dc.load(product=['ls8_fc_albers'], **c3_query, group_by=\"solar_day\", measurements=['BS', 'PV', 'NPV'], \n",
    "                     dask_chunks={'time':1})\n",
    "    c2_wofs = dc.load(product=['wofs_albers'], **c3_query, group_by=\"solar_day\", platform='LANDSAT_8', measurements=['water'],\n",
    "                      dask_chunks={'time':1})\n",
    "    dates = np.intersect1d(c2_ls8.time.data, c2_wofs.time.data)\n",
    "    water = c2_wofs.water & 0b1110_1011\n",
    "    dry = water == 0\n",
    "    c2_ls8 = c2_ls8.loc[dict(time=dates)]\n",
    "    dry = dry.loc[dict(time=dates)]\n",
    "    c2_ls8 = c2_ls8.where((c2_ls8 > -1) & dry & c2_land_raster)\n",
    "\n",
    "    ls7_pc_10 = c2_ls7.quantile([0.2, 0.5, 0.8], dim=['x', 'y'], skipna=True).compute().dropna('time', how='all')\n",
    "    ls8_pc_10 = c2_ls8.quantile([0.2, 0.5, 0.8], dim=['x', 'y'], skipna=True).compute().dropna('time', how='all')\n",
    "    ls7_pc_10 = ls7_pc_10.to_dataframe()\n",
    "    ls8_pc_10 = ls8_pc_10.to_dataframe()\n",
    "    ls7_pc_10.to_csv(grid.lower() + '_fc_ls7_c2.csv')\n",
    "    ls8_pc_10.to_csv(grid.lower() + '_fc_ls8_c2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3,  sharey=True, sharex=True, figsize=(15, 15))\n",
    "i = 0\n",
    "j = 0\n",
    "year = 2014\n",
    "pcs = [0.2, 0.5, 0.8]\n",
    "for y in range(2014, 2021):\n",
    "    data_to_plot = []\n",
    "    for p in pcs:\n",
    "        data_to_plot += [ls7_pc_c2.PV.loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')],\n",
    "                         ls8_pc_c2.PV.loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')]]\n",
    "    box_plot = axs[i, j].boxplot(data_to_plot,\n",
    "                  positions=[1, 1.6, 2.5, 3.1, 4, 4.6],\n",
    "                  labels=['LS7-20','LS8-20','LS7-50','LS8-50','LS7-80','LS8-80'])\n",
    "    axs[i, j].set_title(str(y))\n",
    "    if j >= 2:\n",
    "        i += 1\n",
    "        j = 0\n",
    "    else:\n",
    "        j += 1\n",
    "plt.savefig(test_grids[0] +'_pv_c2_yearly_diff.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"y\": -1, \"x\": -1}\n",
    "\n",
    "for grid in test_grids:\n",
    "    query_poly = poly_from_region_code(grid, \"../../../au-grid.geojson\")\n",
    "    c3_query = {'geopolygon': query_poly}\n",
    "    c3_query['time'] = ('2014-01-01', '2021-01-01')\n",
    "    geobox = GeoBox.from_geopolygon(query_poly, (-30, 30), crs='epsg:3577')\n",
    "    \n",
    "    c3_ls7_datasets = dc.find_datasets(product=['ga_ls7e_ard_3'], **c3_query)\n",
    "    c3_ls7 = load_with_native_transform(\n",
    "        c3_ls7_datasets,\n",
    "        bands=[\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\", \"fmask\", \"nbart_contiguity\"],\n",
    "        geobox=geobox,\n",
    "        native_transform=_native_tr_nbart,\n",
    "        fuser=_fuser_nbart,\n",
    "        groupby=\"solar_day\",\n",
    "        resampling=\"nearest\",\n",
    "        chunks=chunks,\n",
    "    )\n",
    "    c3_land_raster = generate_seamask(\"../../../aus_map/cstauscd_r_3577.shp\",\n",
    "                                          c3_ls7.blue.shape[1:], (c3_ls7.x.data.min(), c3_ls7.y.data.max()), (30, -30))\n",
    "    \n",
    "    c3_ls8_datasets = dc.find_datasets(product='ga_ls8c_ard_3', **c3_query)\n",
    "    c3_ls8 = load_with_native_transform(\n",
    "        c3_ls8_datasets,\n",
    "        bands=[\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\", \"fmask\", \"nbart_contiguity\"],\n",
    "        geobox=geobox,\n",
    "        native_transform=_native_tr_nbart,\n",
    "        fuser=_fuser_nbart,\n",
    "        groupby=\"solar_day\",\n",
    "        resampling=\"nearest\",\n",
    "        chunks=chunks,\n",
    "    )\n",
    "    c3_ls7 = c3_ls7.where((c3_ls7 > -999) & c3_land_raster)\n",
    "    c3_ls8 = c3_ls8.where((c3_ls8 > -999) & c3_land_raster)\n",
    "    ls7_pc_10 = c3_ls7.quantile(pcs, dim=['x', 'y'], skipna=True).compute().dropna('spec', how='all')\n",
    "    ls8_pc_10 = c3_ls8.quantile(pcs, dim=['x', 'y'], skipna=True).compute().dropna('spec', how='all')\n",
    "    ls7_pc_10 = ls7_pc_10.reset_index(['time', 'idx', 'uuid', 'grid'], drop=True).rename({'spec': 'time'}).to_dataframe()\n",
    "    ls8_pc_10 = ls8_pc_10.reset_index(['time', 'idx', 'uuid', 'grid'], drop=True).rename({'spec': 'time'}).to_dataframe()\n",
    "    ls7_pc_10.to_csv(grid.lower() + '_nbart_ls7_c3.csv')\n",
    "    ls8_pc_10.to_csv(grid.lower() + '_nbart_ls8_c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3,  sharey=True, sharex=True, figsize=(15, 15))\n",
    "i = 0\n",
    "j = 0\n",
    "year = 2014\n",
    "for a, b in zip(ls7_pc, ls8_pc):\n",
    "    data_to_plot = []\n",
    "    for _a, _b in zip(a.swir1, b.swir1):\n",
    "        data_to_plot += [_a, _b]\n",
    "    box_plot = axs[i, j].boxplot(data_to_plot, showfliers=False,\n",
    "                  positions=[1, 1.6, 2.5, 3.1, 4, 4.6],\n",
    "                  labels=['LS7-20','LS8-20','LS7-50','LS8-50','LS7-80','LS8-80'])\n",
    "    axs[i, j].set_title(str(year))\n",
    "    year += 1\n",
    "    if j >= 2:\n",
    "        i += 1\n",
    "        j = 0\n",
    "    else:\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf_bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\"]\n",
    "pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "for grid in test_grids:\n",
    "    grid = grid.lower()\n",
    "    ls8_pc = pd.read_csv('nbart_diff_data/'+grid+'_nbart_ls8_c3.csv')\n",
    "    ls8_pc['time'] = ls8_pc['time'].astype(np.datetime64)\n",
    "    ls8_pc = ls8_pc.set_index(['quantile', 'time'])\n",
    "    ls7_pc = pd.read_csv('nbart_diff_data/'+grid+'_nbart_ls7_c3.csv')\n",
    "    ls7_pc['time'] = ls7_pc['time'].astype(np.datetime64)\n",
    "    ls7_pc = ls7_pc.set_index(['quantile', 'time'])\n",
    "    fig, axs = plt.subplots(2, 3,  sharey=True, sharex=True, figsize=(15, 10))\n",
    "    data_to_plot = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for band in sf_bands:\n",
    "        for p in pcs:\n",
    "            data_to_plot = []\n",
    "            for y in range(2014, 2021):\n",
    "                data_to_plot += [ls8_pc[band].loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')].median() - \n",
    "                                 ls7_pc[band].loc[p].loc[slice(str(y)+'-01-01', str(y+1)+'-01-01')].median()]\n",
    "            axs[j, i].plot(np.arange(2014, 2021), data_to_plot, label=\"Median difference on %s percentile\" %  int(p*100))\n",
    "        axs[j, i].axhline(y=0, color='black', linestyle='dashdot')\n",
    "        axs[j, i].set_title(band)\n",
    "        if i >= 2:\n",
    "            j += 1\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "    plt.tight_layout()\n",
    "    handles, labels = axs[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper left', ncol=3)\n",
    "    plt.savefig('nbart_diff_plot/' + grid +'_nbar_c3_yearly_median_diff.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for band in sf_bands:\n",
    "    for q in range(3):\n",
    "        data_to_plot = []\n",
    "        for a, b in zip(ls7_pc, ls8_pc):\n",
    "            data_to_plot += [np.median(b[band].data[q]) - np.median(a[band].data[q])]\n",
    "        axs[j, i].plot(np.arange(2014, 2021), data_to_plot, label=\"Median difference on %s percentile\" %  pcs[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "for grid in test_grids:\n",
    "    grid = grid.lower()\n",
    "    ls8_pc = pd.read_csv('nbart_diff_data/'+grid+'_nbart_ls8_c3.csv')\n",
    "    ls8_pc['time'] = ls8_pc['time'].astype(np.datetime64)\n",
    "    ls8_pc = ls8_pc.set_index(['quantile', 'time'])\n",
    "    ls7_pc = pd.read_csv('nbart_diff_data/'+grid+'_nbart_ls7_c3.csv')\n",
    "    ls7_pc['time'] = ls7_pc['time'].astype(np.datetime64)\n",
    "    ls7_pc = ls7_pc.set_index(['quantile', 'time'])\n",
    "    fig, axs = plt.subplots(2, 3,  sharey=True, sharex=True, figsize=(15, 10))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for band in sf_bands:\n",
    "        pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "        positions = np.arange(1, (len(pcs) + 1), 1.0)\n",
    "        labels_ls7 = ['LS7-'+str(int(p*100)) for p in pcs]\n",
    "        data_to_plot = []\n",
    "        for p in pcs:\n",
    "            data_to_plot += [ls7_pc[band].loc[p]]\n",
    "        box_plot_ls7 = axs[j, i].boxplot(data_to_plot, vert=1, widths=0.3, patch_artist=True, showfliers=False,\n",
    "                      positions=positions,\n",
    "                      labels=labels_ls7)\n",
    "\n",
    "        axs[j, i].set_xticks(list(positions) + list(positions+0.4))\n",
    "        positions += 0.4\n",
    "        labels_ls8 = ['LS8-'+str(int(p*100)) for p in pcs]\n",
    "        data_to_plot = []\n",
    "        for p in pcs:\n",
    "            data_to_plot += [ls8_pc[band].loc[p]]\n",
    "        box_plot_ls8 = axs[j, i].boxplot(data_to_plot, vert=1, widths=0.3, showfliers=False,\n",
    "                      positions=positions,\n",
    "                      labels=labels_ls8)\n",
    "        for item in ['boxes', 'whiskers', 'caps']:\n",
    "            plt.setp(box_plot_ls7[item], color='darkblue')\n",
    "        axs[j, i].set_xticks(list(positions) + list(positions-0.4))\n",
    "        axs[j, i].set_xticklabels(labels_ls8+labels_ls7,\n",
    "                                  rotation=90, fontsize=8)\n",
    "        axs[j, i].set_title(band)\n",
    "        if i >= 2:\n",
    "            j += 1\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "    plt.savefig('nbart_diff_plot/' + grid +'_nbar_alltime_diff.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf_bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\"]\n",
    "pcs = [np.around(a, 1) for a in np.arange(0.1, 1, 0.1)]\n",
    "fig, axs = plt.subplots(2, 3,  sharey=True, sharex=True, figsize=(12, 8))\n",
    "max_median_ls8 = {}\n",
    "max_median_ls7 = {}\n",
    "nbart_lin ={}\n",
    "for band in sf_bands:\n",
    "    max_median_ls8[band] = []\n",
    "    max_median_ls7[band] = []\n",
    "for grid in test_grids:\n",
    "    grid = grid.lower()\n",
    "    ls8_pc = pd.read_csv('nbart_diff_data/'+grid+'_nbart_ls8_c3.csv')\n",
    "    ls8_pc['time'] = ls8_pc['time'].astype(np.datetime64)\n",
    "    ls8_pc = ls8_pc.set_index(['quantile', 'time'])\n",
    "    ls7_pc = pd.read_csv('nbart_diff_data/'+grid+'_nbart_ls7_c3.csv')\n",
    "    ls7_pc['time'] = ls7_pc['time'].astype(np.datetime64)\n",
    "    ls7_pc = ls7_pc.set_index(['quantile', 'time'])\n",
    "    for band in sf_bands:\n",
    "        for p in pcs:\n",
    "            max_median_ls8[band] += [ls8_pc.loc[p][band].median()]\n",
    "            max_median_ls7[band] += [ls7_pc.loc[p][band].median()]\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for band in sf_bands:\n",
    "    nbart_lin[band] = linregress(max_median_ls8[band], max_median_ls7[band])\n",
    "    axs[j, i].plot(max_median_ls8[band], max_median_ls7[band], 'o', color='SteelBlue',  mfc='none', markersize=5)\n",
    "    axs[j, i].plot(max_median_ls8[band], nbart_lin[band].slope*np.array(max_median_ls8[band])+nbart_lin[band].intercept, color='darkorange',\n",
    "                   label=\"regression\")\n",
    "    axs[j, i].axline([0, 0], [1, 1], color='darkgreen', ls='--', label=\"1:1\")\n",
    "    axs[j, i].set_title(band)\n",
    "    axs[j, i].set_xlabel(\"LS8\")\n",
    "    axs[j, i].set_ylabel(\"LS7\")\n",
    "    if i >= 2:\n",
    "        j += 1\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "handles, labels = axs[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper left', ncol=1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('nbart_diff_plot/' + 'nbart_regression.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbart_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "chunks = {\"y\": -1, \"x\": -1}\n",
    "\n",
    "for grid in test_grids:\n",
    "    query_poly = poly_from_region_code(grid, \"../../../au-grid.geojson\")\n",
    "    c3_query = {'geopolygon': query_poly}\n",
    "    c3_query['time'] = ('2014-01-01', '2021-01-01')\n",
    "    geobox = GeoBox.from_geopolygon(query_poly, (-30, 30), crs='epsg:3577')\n",
    "    \n",
    "    c3_ls8_datasets = dc.find_datasets(product='ga_ls8c_ard_3', **c3_query)\n",
    "    c3_ls8 = load_with_native_transform(\n",
    "        c3_ls8_datasets,\n",
    "        bands=[\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\", \"fmask\", \"nbart_contiguity\"],\n",
    "        geobox=geobox,\n",
    "        native_transform=_native_tr_nbart,\n",
    "        fuser=_fuser_nbart,\n",
    "        groupby=\"solar_day\",\n",
    "        resampling=\"nearest\",\n",
    "        chunks=chunks,\n",
    "    )\n",
    "    \n",
    "    c3_land_raster = generate_seamask(\"../../../aus_map/cstauscd_r_3577.shp\",\n",
    "                                          c3_ls8.blue.shape[1:], (c3_ls8.x.data.min(), c3_ls8.y.data.max()), (30, -30))\n",
    "        \n",
    "    c3_ls8 = c3_ls8.where((c3_ls8 > -999) & c3_land_raster)\n",
    "    for key, value in nbart_lin.items():\n",
    "        c3_ls8[key] = c3_ls8[key] * value.slope + value.intercept\n",
    "    c3_ls8 = c3_ls8.clip(min=0)\n",
    "    ls8_pc_10 = c3_ls8.quantile(pcs, dim=['x', 'y'], skipna=True).compute().dropna('spec', how='all')\n",
    "    ls8_pc_10 = ls8_pc_10.reset_index(['time', 'idx', 'uuid', 'grid'], drop=True).rename({'spec': 'time'}).to_dataframe()\n",
    "    ls8_pc_10.to_csv('nbart_diff_data/' + grid.lower() + '_nbart_ls8_c3_lt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "data_to_plot = []\n",
    "for a, b in zip(ls7_box, ls8_box):\n",
    "    data_to_plot += [a, b]\n",
    "box_plot = plt.boxplot(data_to_plot, showfliers=False,\n",
    "                  positions=[1, 1.6, 2.5, 3.1, 4, 4.6],\n",
    "                  labels=['LS7-20','LS8-20','LS7-50','LS8-50','LS7-80','LS8-80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title too long for C2, drop spatial_ref: 3577\n",
    "re_c2 = re_c2.drop_vars('spatial_ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_c2.PV_PC_10.loc[dict(time='2014-01-01')].where(c2_land_raster > 0).compute().plot(aspect=1.5, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the valid data for a band\n",
    "(re_c2.PV_PC_10.loc[dict(time='2014-01-01')]-re_c2.PV_PC_10.loc[dict(time='2010-01-01')]).where(c2_land_raster > 0).compute().plot(aspect=1.5, size=10)\n",
    "#plt.savefig('x45y17_2018_c2.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** August 2021\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tags**: :index:`sandbox compatible`, :index:`landsat 8`, :index:`landsat 7`, :index: `landsat 5`, :index: `fc percentile`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
